{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11552389,"sourceType":"datasetVersion","datasetId":7244345},{"sourceId":356130,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":296957,"modelId":317572},{"sourceId":356134,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":296959,"modelId":317574}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile utils.py\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt\nimport math\n\ndef generate_betas(beta_0, beta_T, num_steps): \n    delta = beta_T - beta_0 \n    return [delta*i/(num_steps-1) + beta_0 for i in range(num_steps)] \n\ndef generate_alpha(beta_0, beta_T, num_steps): \n    betas = generate_betas(beta_0, beta_T, num_steps)\n    alphas = [1-b for b in betas]\n    return alphas\n\ndef generate_alpha_bar(beta_0, beta_T, num_steps): \n    alphas = generate_alpha(beta_0, beta_T, num_steps)\n    alpha_bar = [alphas[0]]\n    for i in range(1, len(alphas)): alpha_bar.append(alpha_bar[i-1] * alphas[i])\n    return alpha_bar\n\ndef count_parameters(model: nn.Module):\n    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total trainable parameters: {total:,}\")\n    return total","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:40:41.022801Z","iopub.execute_input":"2025-05-05T21:40:41.023148Z","iopub.status.idle":"2025-05-05T21:40:41.029930Z","shell.execute_reply.started":"2025-05-05T21:40:41.023123Z","shell.execute_reply":"2025-05-05T21:40:41.028942Z"}},"outputs":[{"name":"stdout","text":"Overwriting utils.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile guided_unet.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef count_parameters(model: nn.Module):\n    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total trainable parameters: {total:,}\")\n    return total\n\n# interleaved = torch.stack((a, b), dim=2).reshape(a.shape[0], -1)\n\ndef get_pos_emb(positions, emb_dim):\n    assert emb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n    factor = 100 ** ((torch.arange(\n        start=0, end=emb_dim // 2, dtype=torch.float32, device=positions.device) / (emb_dim // 2))\n    )\n    t_emb = positions[:, None]\n    t_emb = t_emb.repeat(1, emb_dim // 2)\n    t_emb = t_emb / factor\n    t_emb = torch.stack((torch.sin(t_emb), torch.cos(t_emb)), dim=2).reshape(t_emb.shape[0], -1)\n    return t_emb\n\ndef get_time_embedding(time_steps, temb_dim):\n    # factor = 10000^(2i/d_model)\n    factor = 100 ** ((torch.arange(\n        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n    )\n    \n    # pos / factor\n    # timesteps B -> B, 1 -> B, temb_dim\n    t_emb = time_steps[:, None].repeat(1, temb_dim // 2)\n    t_emb = t_emb / factor\n    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n    return t_emb\n\n\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample=True, num_heads=4, num_layers=1, X_att_dim=768):\n        super().__init__()\n        self.num_layers = num_layers\n        self.down_sample = down_sample\n        \n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        \n        self.t_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(t_emb_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.X_att_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(X_att_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(8, out_channels)\n             for _ in range(num_layers)]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n             for _ in range(num_layers)]\n        )\n        self.Xatts = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        \n        self.down_sample_conv = nn.Conv2d(out_channels, out_channels, 4, 2, 1) if self.down_sample else nn.Identity()\n        self.lamb_pre = nn.Parameter(torch.tensor([0.5]))\n\n    def forward(self, x, t_emb, X_att_emb, guided):\n        out = x\n        for i in range(self.num_layers):\n            \n            # Resnet block of Unet\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            # Attention block of Unet\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n\n            X_att_emb_i = self.X_att_emb_layers[i](X_att_emb)\n            out_attn_self, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn_cross, _ = self.Xatts[i](in_attn, X_att_emb_i, X_att_emb_i)\n            out_attn = out_attn_self + F.tanh(self.lamb_pre) * out_attn_cross * guided.view(-1, 1, 1)\n\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n            \n        out = self.down_sample_conv(out)\n        return out\n\n\nclass MidBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1, X_att_dim=768):\n        super().__init__()\n        self.num_layers = num_layers\n        \n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers+1)\n            ]\n        )\n        \n        self.t_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(t_emb_dim, out_channels)\n            )\n            for _ in range(num_layers + 1)\n        ])\n        self.X_att_emb_layer = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(X_att_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers+1)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(8, out_channels)\n                for _ in range(num_layers)]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)]\n        )\n        self.Xatts = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers+1)\n            ]\n        )\n\n        self.lamb_pre = nn.Parameter(torch.tensor([0.5]))\n    \n    def forward(self, x, t_emb, X_att_emb, guided):\n        out = x\n        \n        # First resnet block\n        resnet_input = out\n        out = self.resnet_conv_first[0](out)\n        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n        out = self.resnet_conv_second[0](out)\n        out = out + self.residual_input_conv[0](resnet_input)\n        \n        for i in range(self.num_layers):\n            \n            # Attention Block\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n\n            X_att_emb_i = self.X_att_emb_layer[i](X_att_emb)\n            out_attn_self, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn_cross, _ = self.Xatts[i](in_attn, X_att_emb_i, X_att_emb_i)\n            out_attn = out_attn_self + F.tanh(self.lamb_pre) * out_attn_cross * guided.view(-1, 1, 1)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n\n            # Resnet Block\n            resnet_input = out\n            out = self.resnet_conv_first[i+1](out)\n            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i+1](out)\n            out = out + self.residual_input_conv[i+1](resnet_input)\n        \n        return out\n\n\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1, X_att_dim=768):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        self.t_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(t_emb_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.X_att_emb_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(X_att_dim, out_channels)\n            )\n            for _ in range(num_layers)\n        ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(8, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )       \n        self.attention_norms = nn.ModuleList(\n            [\n                nn.GroupNorm(8, out_channels)\n                for _ in range(num_layers)\n            ]\n        )   \n        self.attentions = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n        self.Xatts = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n                                                 4, 2, 1) \\\n            if self.up_sample else nn.Identity()\n        \n        self.lamb_pre = nn.Parameter(torch.tensor([0.5]))\n    \n    def forward(self, x, out_down, t_emb, X_att_emb, guided):\n        x = self.up_sample_conv(x)\n        x = torch.cat([x, out_down], dim=1)\n\n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n\n            X_att_emb_i = self.X_att_emb_layers[i](X_att_emb)\n            out_attn_self, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn_cross, _ = self.Xatts[i](in_attn, X_att_emb_i, X_att_emb_i)\n            out_attn = out_attn_self + F.tanh(self.lamb_pre) * out_attn_cross * guided.view(-1, 1, 1)\n\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n\n        return out\n\n\nclass GuidedUnet(nn.Module):\n    def __init__(self, X_att_dim):\n        super().__init__()\n        im_channels = 3\n        self.down_channels = [32, 64, 128, 128]\n        self.mid_channels = [128, 128, 128]\n        self.t_emb_dim = 128\n        self.down_sample = [True, True, False]\n        self.num_down_layers = 2\n        self.num_mid_layers = 2\n        self.num_up_layers = 2\n        self.X_att_dim = X_att_dim\n        self.max_tokens = 20\n             \n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-2]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        \n        # Initial projection from sinusoidal time embedding\n        self.t_proj = nn.Sequential(\n            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n            nn.SiLU(),\n            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n        )\n\n        self.up_sample = list(reversed(self.down_sample))\n        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n        \n        self.downs = nn.ModuleList([])\n        for i in range(len(self.down_channels)-1):\n            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers, X_att_dim=X_att_dim))\n        \n        self.mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels)-1):\n            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n                                      num_layers=self.num_mid_layers, X_att_dim=X_att_dim))\n        \n        self.ups = nn.ModuleList([])\n        for i in reversed(range(len(self.down_channels)-1)):\n            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers, X_att_dim=X_att_dim))\n        \n        self.norm_out = nn.GroupNorm(8, 16)\n        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n        self.pos_enc = nn.Parameter(torch.tensor(get_pos_emb(torch.tensor(range(self.max_tokens)), self.X_att_dim)))\n\n    def forward(self, x, t, xa_emb, guided):\n        out = self.conv_in(x)\n        # B x C1 x H x W\n\n        xa_emb = xa_emb + self.pos_enc[:xa_emb.shape[1]]\n        # w_emb = self.text_enc(w_emb)\n        \n        # t_emb -> B x t_emb_dim\n        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n        t_emb = self.t_proj(t_emb)\n        \n        down_outs = []\n        \n        for idx, down in enumerate(self.downs):\n            down_outs.append(out)\n            out = down(out, t_emb, xa_emb, guided)\n        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n        # out B x C4 x H/4 x W/4\n            \n        for mid in self.mids:\n            out = mid(out, t_emb, xa_emb, guided)\n        # out B x C3 x H/4 x W/4\n        \n        for up in self.ups:\n            down_out = down_outs.pop()\n            out = up(out, down_out, t_emb, xa_emb, guided)\n            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n\n        out = self.norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.conv_out(out)\n        # out B x C x H x W\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:56:12.127553Z","iopub.execute_input":"2025-05-05T21:56:12.128286Z","iopub.status.idle":"2025-05-05T21:56:12.139515Z","shell.execute_reply.started":"2025-05-05T21:56:12.128255Z","shell.execute_reply":"2025-05-05T21:56:12.138899Z"}},"outputs":[{"name":"stdout","text":"Overwriting guided_unet.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"%%writefile runme.py\nimport torch\nimport torch.nn as nn \nfrom torch.utils.data import TensorDataset, DataLoader\nfrom tqdm import tqdm\nfrom torch.utils.data import Subset\nfrom tqdm import tqdm\nfrom guided_unet import GuidedUnet, get_pos_emb\nfrom utils import count_parameters\ndevice = torch.device(\"cuda\")\n\n# IMPORT WORD EMBEDDING MATRIX\ndim = 768\nmym = GuidedUnet(dim).to(device)\ncount_parameters(mym)\n\n# IMPORT DATASET\nlatent_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/z_set.pth\")\ntime_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/t_set.pth\")\nlabel_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/l_set.pth\")\nnoise_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/n_set.pth\")\nprint(latent_set.shape, time_set.shape,label_set.shape, noise_set.shape)\n\ndataset = TensorDataset(latent_set, time_set, label_set, noise_set)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\nprint(\"train_loader created\")\n\nmax_label = int(torch.max(label_set).item())\nprint(\"max_label:\", max_label)\nemb_mat = get_pos_emb(torch.tensor(range(max_label+1)), dim).to(device)\n\nprint(\"beginning training\")\ncriterion = nn.MSELoss()\nmym = mym.to(device)\noptimizer = torch.optim.Adam(mym.parameters(), lr=5e-5)\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n    iter = 0\n\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n    last_loss = 0.0  # To keep track of the last batch's loss\n    running_loss = 0\n\n    for latent, time, label, noise in pbar:\n        latent, time, label, noise = latent.to(device), time.to(device), label.to(device), noise.to(device)\n        B = latent.shape[0]\n        iter += 1\n\n        guided = torch.randint(0,2,(B,)).cuda()\n\n        x_att_in = emb_mat[label.long()].unsqueeze(1).repeat(1,5,1)\n\n        # print(latent.shape, time.shape, x_att_in.shape, guided.shape)\n        ns_hat = mym(latent, time, x_att_in, guided)\n        loss = criterion(ns_hat, noise)\n        \n        # BATCH STUFF\n        optimizer.zero_grad()\n        loss.backward()    \n        optimizer.step()\n        total_loss += loss.item()\n        last_loss = loss.item() \n        running_loss = 0.98 * running_loss + 0.02 * last_loss\n        if iter % 100 == 0: torch.save(mym, \"noise_predictor_b.pth\")\n        pbar.set_postfix(bl=1000*last_loss, rl=1000*running_loss)\n\n    print(f\"Epoch {epoch}, Last Batch Loss: {last_loss}, Total Loss: {total_loss:.4f}\")\n    torch.save(mym, \"noise_predictor_e.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:57:17.615089Z","iopub.execute_input":"2025-05-05T21:57:17.615868Z","iopub.status.idle":"2025-05-05T21:57:17.621581Z","shell.execute_reply.started":"2025-05-05T21:57:17.615837Z","shell.execute_reply":"2025-05-05T21:57:17.620904Z"}},"outputs":[{"name":"stdout","text":"Overwriting runme.py\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!python runme.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:57:19.850122Z","iopub.execute_input":"2025-05-05T21:57:19.850804Z","iopub.status.idle":"2025-05-05T21:59:36.739393Z","shell.execute_reply.started":"2025-05-05T21:57:19.850778Z","shell.execute_reply":"2025-05-05T21:59:36.738731Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/guided_unet.py:381: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.pos_enc = nn.Parameter(torch.tensor(get_pos_emb(torch.tensor(range(self.max_tokens)), self.X_att_dim)))\nTotal trainable parameters: 6,485,883\n/kaggle/working/runme.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  latent_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/z_set.pth\")\n/kaggle/working/runme.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  time_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/t_set.pth\")\n/kaggle/working/runme.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  label_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/l_set.pth\")\n/kaggle/working/runme.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  noise_set = torch.load(\"/kaggle/input/fruit-diffusion-dataset/n_set.pth\")\ntorch.Size([170000, 3, 32, 32]) torch.Size([170000]) torch.Size([170000]) torch.Size([170000, 3, 32, 32])\ntrain_loader created\nmax_label: 5\nbeginning training\nEpoch 0:  13%|█▉             | 694/5313 [02:09<14:15,  5.40it/s, bl=215, rl=245]^C\nEpoch 0:  13%|█▉             | 695/5313 [02:10<14:25,  5.34it/s, bl=187, rl=244]\nTraceback (most recent call last):\n  File \"/kaggle/working/runme.py\", line 60, in <module>\n    loss.backward()    \n    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":34}]}